{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Using Cross-Validation for an Advanced Fibrosis Diagnosis Classifier\n",
    "\n",
    "We learned about the hepatitis C dataset in Activity 3.02, Advanced Fibrosis Diagnosis with Neural Networks of Chapter 3, Deep Learning with Keras. The dataset consists of information for 1385 patients who underwent treatment dosages for hepatitis C. For each patient, 28 different attributes are available, such as age, gender, and BMI, as well as a class label, which can only take two values: 1, indicating advanced fibrosis, and 0, indicating no indication of advanced fibrosis. This is a binary/two-class classification problem with an input dimension equal to 28.\n",
    "\n",
    "In Chapter 3, Deep Learning with Keras, we built Keras models to perform classification on this dataset. We trained and evaluated the models using training set/test set splitting and reported the test error rate. In this activity, we are going to use what we learned in this topic to train and evaluate a deep learning model using k-fold cross-validation. We will use the model that resulted in the best test error rate from the previous activity. The goal is to compare the cross-validation error rate with the training set/test set approach error rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the necessary libraries. Load the dataset from the data subfolder of the Chapter04 folder from GitHub using X = pd.read_csv('../data/HCV_feats.csv'), y = pd.read_csv('../data/HCV_target.csv'). Print the number of examples in the dataset, the number of features available, and the possible values for the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1385, 28)\n(1385, 1)\n"
    }
   ],
   "source": [
    "X = pd.read_csv('../data/HCV_feats.csv')\n",
    "y = pd.read_csv('../data/HCV_target.csv')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    717\n0    668\nName: AdvancedFibrosis, dtype: int64"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "y.AdvancedFibrosis.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the function that returns the Keras model. The Keras model will be a deep neural network with two hidden layers, where the first hidden layer is of size 4 and the second hidden layer is of size 2, and use the tanh activation function to perform the classification. Use the following values for the hyperparameters: optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(4, input_dim=X.shape[1], activation='tanh'))\n",
    "    model.add(Dense(2, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = 'adam', \n",
    "        loss = 'binary_crossentropy', \n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the scikit-learn interface for the Keras model with epochs=100, batch_size=20, and shuffle=False. Define the cross-validation iterator as StratifiedKFold with k=5. Perform k-fold cross-validation on the model and store the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random\n",
    "\n",
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "h 14/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6763 - accuracy: 0.5632\nEpoch 15/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6748 - accuracy: 0.5650\nEpoch 16/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6736 - accuracy: 0.5632\nEpoch 17/100\n1108/1108 [==============================] - 0s 156us/step - loss: 0.6724 - accuracy: 0.5641\nEpoch 18/100\n1108/1108 [==============================] - 0s 138us/step - loss: 0.6714 - accuracy: 0.5668\nEpoch 19/100\n1108/1108 [==============================] - 0s 131us/step - loss: 0.6703 - accuracy: 0.5713\nEpoch 20/100\n1108/1108 [==============================] - 0s 140us/step - loss: 0.6694 - accuracy: 0.5767\nEpoch 21/100\n1108/1108 [==============================] - 0s 130us/step - loss: 0.6685 - accuracy: 0.5740\nEpoch 22/100\n1108/1108 [==============================] - 0s 133us/step - loss: 0.6676 - accuracy: 0.5722\nEpoch 23/100\n1108/1108 [==============================] - 0s 122us/step - loss: 0.6667 - accuracy: 0.5704\nEpoch 24/100\n1108/1108 [==============================] - 0s 121us/step - loss: 0.6659 - accuracy: 0.5704\nEpoch 25/100\n1108/1108 [==============================] - 0s 170us/step - loss: 0.6651 - accuracy: 0.5722\nEpoch 26/100\n1108/1108 [==============================] - 0s 120us/step - loss: 0.6643 - accuracy: 0.5722\nEpoch 27/100\n1108/1108 [==============================] - 0s 170us/step - loss: 0.6635 - accuracy: 0.5713\nEpoch 28/100\n1108/1108 [==============================] - 0s 136us/step - loss: 0.6627 - accuracy: 0.5713\nEpoch 29/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6619 - accuracy: 0.5704\nEpoch 30/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6611 - accuracy: 0.5695\nEpoch 31/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6604 - accuracy: 0.5686\nEpoch 32/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6597 - accuracy: 0.5668\nEpoch 33/100\n1108/1108 [==============================] - 0s 120us/step - loss: 0.6589 - accuracy: 0.5659\nEpoch 34/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6582 - accuracy: 0.5650\nEpoch 35/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6575 - accuracy: 0.5677\nEpoch 36/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6568 - accuracy: 0.5695\nEpoch 37/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6561 - accuracy: 0.5767\nEpoch 38/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6554 - accuracy: 0.5785\nEpoch 39/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6546 - accuracy: 0.5794\nEpoch 40/100\n1108/1108 [==============================] - 0s 113us/step - loss: 0.6539 - accuracy: 0.5785\nEpoch 41/100\n1108/1108 [==============================] - 0s 122us/step - loss: 0.6532 - accuracy: 0.5794\nEpoch 42/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6525 - accuracy: 0.5785\nEpoch 43/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6517 - accuracy: 0.5821\nEpoch 44/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6510 - accuracy: 0.5803\nEpoch 45/100\n1108/1108 [==============================] - 0s 111us/step - loss: 0.6503 - accuracy: 0.5794\nEpoch 46/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6495 - accuracy: 0.5803\nEpoch 47/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6488 - accuracy: 0.5812\nEpoch 48/100\n1108/1108 [==============================] - 0s 121us/step - loss: 0.6480 - accuracy: 0.5812\nEpoch 49/100\n1108/1108 [==============================] - 0s 111us/step - loss: 0.6473 - accuracy: 0.5848\nEpoch 50/100\n1108/1108 [==============================] - 0s 116us/step - loss: 0.6465 - accuracy: 0.5848\nEpoch 51/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6457 - accuracy: 0.5866\nEpoch 52/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6450 - accuracy: 0.5857\nEpoch 53/100\n1108/1108 [==============================] - 0s 142us/step - loss: 0.6442 - accuracy: 0.5839\nEpoch 54/100\n1108/1108 [==============================] - 0s 149us/step - loss: 0.6434 - accuracy: 0.5812\nEpoch 55/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6427 - accuracy: 0.5812\nEpoch 56/100\n1108/1108 [==============================] - 0s 125us/step - loss: 0.6419 - accuracy: 0.5839\nEpoch 57/100\n1108/1108 [==============================] - 0s 135us/step - loss: 0.6412 - accuracy: 0.5857\nEpoch 58/100\n1108/1108 [==============================] - 0s 132us/step - loss: 0.6404 - accuracy: 0.5866\nEpoch 59/100\n1108/1108 [==============================] - 0s 129us/step - loss: 0.6397 - accuracy: 0.5875\nEpoch 60/100\n1108/1108 [==============================] - 0s 139us/step - loss: 0.6390 - accuracy: 0.5875\nEpoch 61/100\n1108/1108 [==============================] - 0s 145us/step - loss: 0.6383 - accuracy: 0.5875\nEpoch 62/100\n1108/1108 [==============================] - 0s 152us/step - loss: 0.6376 - accuracy: 0.5857\nEpoch 63/100\n1108/1108 [==============================] - 0s 113us/step - loss: 0.6369 - accuracy: 0.5857\nEpoch 64/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6362 - accuracy: 0.5903\nEpoch 65/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6356 - accuracy: 0.5903\nEpoch 66/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6350 - accuracy: 0.5921\nEpoch 67/100\n1108/1108 [==============================] - 0s 143us/step - loss: 0.6343 - accuracy: 0.5930\nEpoch 68/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6337 - accuracy: 0.5939\nEpoch 69/100\n1108/1108 [==============================] - 0s 113us/step - loss: 0.6331 - accuracy: 0.5930\nEpoch 70/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6325 - accuracy: 0.5948\nEpoch 71/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6319 - accuracy: 0.5939\nEpoch 72/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6313 - accuracy: 0.5939\nEpoch 73/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6308 - accuracy: 0.5966\nEpoch 74/100\n1108/1108 [==============================] - 0s 197us/step - loss: 0.6302 - accuracy: 0.5975\nEpoch 75/100\n1108/1108 [==============================] - 0s 129us/step - loss: 0.6296 - accuracy: 0.5975\nEpoch 76/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6291 - accuracy: 0.6011\nEpoch 77/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6285 - accuracy: 0.6002\nEpoch 78/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6279 - accuracy: 0.6020\nEpoch 79/100\n1108/1108 [==============================] - 0s 109us/step - loss: 0.6274 - accuracy: 0.6020\nEpoch 80/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6268 - accuracy: 0.6029\nEpoch 81/100\n1108/1108 [==============================] - 0s 129us/step - loss: 0.6263 - accuracy: 0.6011\nEpoch 82/100\n1108/1108 [==============================] - 0s 123us/step - loss: 0.6257 - accuracy: 0.6020\nEpoch 83/100\n1108/1108 [==============================] - 0s 170us/step - loss: 0.6251 - accuracy: 0.6011\nEpoch 84/100\n1108/1108 [==============================] - 0s 138us/step - loss: 0.6246 - accuracy: 0.6020\nEpoch 85/100\n1108/1108 [==============================] - 0s 121us/step - loss: 0.6240 - accuracy: 0.6011\nEpoch 86/100\n1108/1108 [==============================] - 0s 118us/step - loss: 0.6234 - accuracy: 0.6029\nEpoch 87/100\n1108/1108 [==============================] - 0s 139us/step - loss: 0.6228 - accuracy: 0.6020\nEpoch 88/100\n1108/1108 [==============================] - 0s 176us/step - loss: 0.6223 - accuracy: 0.6020\nEpoch 89/100\n1108/1108 [==============================] - 0s 133us/step - loss: 0.6217 - accuracy: 0.5993\nEpoch 90/100\n1108/1108 [==============================] - 0s 126us/step - loss: 0.6211 - accuracy: 0.6011\nEpoch 91/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6205 - accuracy: 0.6029\nEpoch 92/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6199 - accuracy: 0.6029\nEpoch 93/100\n1108/1108 [==============================] - 0s 134us/step - loss: 0.6194 - accuracy: 0.6047\nEpoch 94/100\n1108/1108 [==============================] - 0s 135us/step - loss: 0.6188 - accuracy: 0.6056\nEpoch 95/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6182 - accuracy: 0.6065\nEpoch 96/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6176 - accuracy: 0.6056\nEpoch 97/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6171 - accuracy: 0.6074\nEpoch 98/100\n1108/1108 [==============================] - 0s 98us/step - loss: 0.6165 - accuracy: 0.6092\nEpoch 99/100\n1108/1108 [==============================] - 0s 104us/step - loss: 0.6159 - accuracy: 0.6092\nEpoch 100/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6154 - accuracy: 0.6110\n277/277 [==============================] - 0s 170us/step\nEpoch 1/100\n1108/1108 [==============================] - 0s 219us/step - loss: 0.7266 - accuracy: 0.5099\nEpoch 2/100\n1108/1108 [==============================] - 0s 89us/step - loss: 0.7161 - accuracy: 0.5171\nEpoch 3/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.7088 - accuracy: 0.5181\nEpoch 4/100\n1108/1108 [==============================] - 0s 118us/step - loss: 0.7034 - accuracy: 0.5217\nEpoch 5/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6993 - accuracy: 0.5271\nEpoch 6/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6962 - accuracy: 0.5280\nEpoch 7/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6937 - accuracy: 0.5343\nEpoch 8/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6917 - accuracy: 0.5352\nEpoch 9/100\n1108/1108 [==============================] - 0s 155us/step - loss: 0.6901 - accuracy: 0.5424\nEpoch 10/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6886 - accuracy: 0.5388\nEpoch 11/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6874 - accuracy: 0.5370\nEpoch 12/100\n1108/1108 [==============================] - 0s 93us/step - loss: 0.6862 - accuracy: 0.5397\nEpoch 13/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6851 - accuracy: 0.5496\nEpoch 14/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6841 - accuracy: 0.5487\nEpoch 15/100\n1108/1108 [==============================] - 0s 100us/step - loss: 0.6831 - accuracy: 0.5560\nEpoch 16/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6821 - accuracy: 0.5596\nEpoch 17/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6811 - accuracy: 0.5650\nEpoch 18/100\n1108/1108 [==============================] - 0s 103us/step - loss: 0.6802 - accuracy: 0.5695\nEpoch 19/100\n1108/1108 [==============================] - 0s 129us/step - loss: 0.6792 - accuracy: 0.5731\nEpoch 20/100\n1108/1108 [==============================] - 0s 128us/step - loss: 0.6783 - accuracy: 0.5749\nEpoch 21/100\n1108/1108 [==============================] - 0s 120us/step - loss: 0.6773 - accuracy: 0.5830\nEpoch 22/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6764 - accuracy: 0.5939\nEpoch 23/100\n1108/1108 [==============================] - 0s 127us/step - loss: 0.6754 - accuracy: 0.5966\nEpoch 24/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6745 - accuracy: 0.6038\nEpoch 25/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6735 - accuracy: 0.6074\nEpoch 26/100\n1108/1108 [==============================] - 0s 93us/step - loss: 0.6725 - accuracy: 0.6083\nEpoch 27/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6716 - accuracy: 0.6128\nEpoch 28/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6706 - accuracy: 0.6128\nEpoch 29/100\n1108/1108 [==============================] - 0s 100us/step - loss: 0.6696 - accuracy: 0.6155\nEpoch 30/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6686 - accuracy: 0.6155\nEpoch 31/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6676 - accuracy: 0.6110\nEpoch 32/100\n1108/1108 [==============================] - 0s 130us/step - loss: 0.6666 - accuracy: 0.6128\nEpoch 33/100\n1108/1108 [==============================] - 0s 135us/step - loss: 0.6656 - accuracy: 0.6110\nEpoch 34/100\n1108/1108 [==============================] - 0s 123us/step - loss: 0.6645 - accuracy: 0.6155\nEpoch 35/100\n1108/1108 [==============================] - 0s 99us/step - loss: 0.6635 - accuracy: 0.6182\nEpoch 36/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6624 - accuracy: 0.6209\nEpoch 37/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6614 - accuracy: 0.6191\nEpoch 38/100\n1108/1108 [==============================] - 0s 110us/step - loss: 0.6603 - accuracy: 0.6218\nEpoch 39/100\n1108/1108 [==============================] - 0s 104us/step - loss: 0.6593 - accuracy: 0.6218\nEpoch 40/100\n1108/1108 [==============================] - 0s 105us/step - loss: 0.6583 - accuracy: 0.6200\nEpoch 41/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6573 - accuracy: 0.6209\nEpoch 42/100\n1108/1108 [==============================] - 0s 122us/step - loss: 0.6563 - accuracy: 0.6236\nEpoch 43/100\n1108/1108 [==============================] - 0s 107us/step - loss: 0.6554 - accuracy: 0.6245\nEpoch 44/100\n1108/1108 [==============================] - 0s 103us/step - loss: 0.6544 - accuracy: 0.6245\nEpoch 45/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6535 - accuracy: 0.6236\nEpoch 46/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6527 - accuracy: 0.6255\nEpoch 47/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6519 - accuracy: 0.6264\nEpoch 48/100\n1108/1108 [==============================] - 0s 123us/step - loss: 0.6510 - accuracy: 0.6291\nEpoch 49/100\n1108/1108 [==============================] - 0s 130us/step - loss: 0.6503 - accuracy: 0.6327\nEpoch 50/100\n1108/1108 [==============================] - 0s 117us/step - loss: 0.6495 - accuracy: 0.6327\nEpoch 51/100\n1108/1108 [==============================] - 0s 103us/step - loss: 0.6488 - accuracy: 0.6354\nEpoch 52/100\n1108/1108 [==============================] - 0s 109us/step - loss: 0.6481 - accuracy: 0.6390\nEpoch 53/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6474 - accuracy: 0.6417\nEpoch 54/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6467 - accuracy: 0.6408\nEpoch 55/100\n1108/1108 [==============================] - 0s 102us/step - loss: 0.6460 - accuracy: 0.6417\nEpoch 56/100\n1108/1108 [==============================] - 0s 104us/step - loss: 0.6454 - accuracy: 0.6417\nEpoch 57/100\n1000/1108 [==========================>...] - ETA: 0s - loss: 0.6433 - accuracy: 0.641108/1108 [==============================] - 0s 110us/step - loss: 0.6447 - accuracy: 0.6408\nEpoch 58/100\n1108/1108 [==============================] - 0s 142us/step - loss: 0.6441 - accuracy: 0.6417\nEpoch 59/100\n1108/1108 [==============================] - 0s 125us/step - loss: 0.6435 - accuracy: 0.6381\nEpoch 60/100\n1108/1108 [==============================] - 0s 123us/step - loss: 0.6429 - accuracy: 0.6390\nEpoch 61/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6423 - accuracy: 0.6372\nEpoch 62/100\n1108/1108 [==============================] - 0s 133us/step - loss: 0.6417 - accuracy: 0.6363\nEpoch 63/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6412 - accuracy: 0.6381\nEpoch 64/100\n1108/1108 [==============================] - 0s 105us/step - loss: 0.6406 - accuracy: 0.6381\nEpoch 65/100\n1108/1108 [==============================] - 0s 109us/step - loss: 0.6401 - accuracy: 0.6417\nEpoch 66/100\n1108/1108 [==============================] - 0s 99us/step - loss: 0.6396 - accuracy: 0.6435\nEpoch 67/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6391 - accuracy: 0.6462\nEpoch 68/100\n1108/1108 [==============================] - 0s 105us/step - loss: 0.6386 - accuracy: 0.6453\nEpoch 69/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6381 - accuracy: 0.6462\nEpoch 70/100\n1108/1108 [==============================] - 0s 168us/step - loss: 0.6376 - accuracy: 0.6444\nEpoch 71/100\n1108/1108 [==============================] - 0s 121us/step - loss: 0.6371 - accuracy: 0.6471\nEpoch 72/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6367 - accuracy: 0.6489\nEpoch 73/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6362 - accuracy: 0.6489\nEpoch 74/100\n1108/1108 [==============================] - 0s 116us/step - loss: 0.6357 - accuracy: 0.6507\nEpoch 75/100\n1108/1108 [==============================] - 0s 95us/step - loss: 0.6353 - accuracy: 0.6498\nEpoch 76/100\n1108/1108 [==============================] - 0s 124us/step - loss: 0.6348 - accuracy: 0.6498\nEpoch 77/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6344 - accuracy: 0.6498\nEpoch 78/100\n1108/1108 [==============================] - 0s 104us/step - loss: 0.6340 - accuracy: 0.6516\nEpoch 79/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6336 - accuracy: 0.6552\nEpoch 80/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6331 - accuracy: 0.6552\nEpoch 81/100\n1108/1108 [==============================] - 0s 93us/step - loss: 0.6327 - accuracy: 0.6561\nEpoch 82/100\n1108/1108 [==============================] - 0s 112us/step - loss: 0.6323 - accuracy: 0.6543\nEpoch 83/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6319 - accuracy: 0.6579\nEpoch 84/100\n1108/1108 [==============================] - 0s 102us/step - loss: 0.6315 - accuracy: 0.6588\nEpoch 85/100\n1108/1108 [==============================] - 0s 119us/step - loss: 0.6311 - accuracy: 0.6570\nEpoch 86/100\n1108/1108 [==============================] - 0s 115us/step - loss: 0.6307 - accuracy: 0.6579\nEpoch 87/100\n1108/1108 [==============================] - 0s 106us/step - loss: 0.6303 - accuracy: 0.6588\nEpoch 88/100\n1108/1108 [==============================] - 0s 91us/step - loss: 0.6299 - accuracy: 0.6588\nEpoch 89/100\n1108/1108 [==============================] - 0s 100us/step - loss: 0.6295 - accuracy: 0.6579\nEpoch 90/100\n1108/1108 [==============================] - 0s 113us/step - loss: 0.6292 - accuracy: 0.6570\nEpoch 91/100\n1108/1108 [==============================] - 0s 108us/step - loss: 0.6288 - accuracy: 0.6588\nEpoch 92/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6284 - accuracy: 0.6588\nEpoch 93/100\n1108/1108 [==============================] - 0s 118us/step - loss: 0.6281 - accuracy: 0.6597\nEpoch 94/100\n1108/1108 [==============================] - 0s 116us/step - loss: 0.6277 - accuracy: 0.6606\nEpoch 95/100\n1108/1108 [==============================] - 0s 114us/step - loss: 0.6273 - accuracy: 0.6616\nEpoch 96/100\n1108/1108 [==============================] - 0s 176us/step - loss: 0.6270 - accuracy: 0.6625\nEpoch 97/100\n1108/1108 [==============================] - 0s 128us/step - loss: 0.6267 - accuracy: 0.6652\nEpoch 98/100\n1108/1108 [==============================] - 0s 134us/step - loss: 0.6263 - accuracy: 0.6661\nEpoch 99/100\n1108/1108 [==============================] - 0s 118us/step - loss: 0.6260 - accuracy: 0.6661\nEpoch 100/100\n1108/1108 [==============================] - 0s 121us/step - loss: 0.6257 - accuracy: 0.6661\n277/277 [==============================] - 0s 198us/step\n"
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# params = {\n",
    "#     build_fn: build_model,\n",
    "#     epochs: 100,\n",
    "#     batch_size: 20,\n",
    "#     shuffle: False,\n",
    "#     verbose: 1\n",
    "# }\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_model, epochs=100, batch_size=20, verbose=1, shuffle=False)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=seed, shuffle=False)\n",
    "\n",
    "scores = cross_val_score(classifier, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Print the accuracy for each iteration/fold, plus the overall cross-validation accuracy and its associated standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.52346569, 0.51624548, 0.53429604, 0.52346569, 0.58844763])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5371841073036194"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.026272059193352466"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we received from training set/test set approach we performed in Activity 3.02, Advanced Fibrosis Diagnosis with Neural Networks of Chapter 3, Deep Learning with Keras, was 49.819%, which is lower than the test accuracy we achieved when performing 5-fold cross-validation on the same deep learning model and the same dataset, but lower than the accuracy on one of the folds.\n",
    "\n",
    "The reason for this difference is that the test error rate resulting from the training set/test set approach was computed by only including a subset of the data points in the model's evaluation. On the other hand, the test error rate here is computed by including all the data points in the evaluation, and therefore this estimation of the model's performance is more accurate and more robust, performing better on the unseen test dataset.\n",
    "\n",
    "In this activity, we used cross-validation to perform a model evaluation on a problem involving a real dataset. Improving model evaluation is not the only purpose of using cross-validation, and it can be used to select the best model or parameters for a given problem as well."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitdlwkvenvbec5b905c558415eb2dad3aba1eb0bd0",
   "display_name": "Python 3.7.2 64-bit ('dlwk': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}