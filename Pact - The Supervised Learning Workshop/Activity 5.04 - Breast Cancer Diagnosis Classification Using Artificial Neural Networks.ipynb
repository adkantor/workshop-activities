{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                   0.07871  ...          17.33           184.60      2019.0   \n1                   0.05667  ...          23.41           158.80      1956.0   \n2                   0.05999  ...          25.53           152.50      1709.0   \n3                   0.09744  ...          26.50            98.87       567.7   \n4                   0.05883  ...          16.67           152.20      1575.0   \n..                      ...  ...            ...              ...         ...   \n564                 0.05623  ...          26.40           166.10      2027.0   \n565                 0.05533  ...          38.25           155.00      1731.0   \n566                 0.05648  ...          34.12           126.70      1124.0   \n567                 0.07016  ...          39.42           184.60      1821.0   \n568                 0.05884  ...          30.37            59.16       268.6   \n\n     worst smoothness  worst compactness  worst concavity  \\\n0             0.16220            0.66560           0.7119   \n1             0.12380            0.18660           0.2416   \n2             0.14440            0.42450           0.4504   \n3             0.20980            0.86630           0.6869   \n4             0.13740            0.20500           0.4000   \n..                ...                ...              ...   \n564           0.14100            0.21130           0.4107   \n565           0.11660            0.19220           0.3215   \n566           0.11390            0.30940           0.3403   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     worst concave points  worst symmetry  worst fractal dimension  diagnosis  \n0                  0.2654          0.4601                  0.11890  malignant  \n1                  0.1860          0.2750                  0.08902  malignant  \n2                  0.2430          0.3613                  0.08758  malignant  \n3                  0.2575          0.6638                  0.17300  malignant  \n4                  0.1625          0.2364                  0.07678  malignant  \n..                    ...             ...                      ...        ...  \n564                0.2216          0.2060                  0.07115  malignant  \n565                0.1628          0.2572                  0.06637  malignant  \n566                0.1418          0.2218                  0.07820  malignant  \n567                0.2650          0.4087                  0.12400  malignant  \n568                0.0000          0.2871                  0.07039     benign  \n\n[569 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>diagnosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>malignant</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.read_csv('../Datasets/breast-cancer-data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is feature engineering. Different columns of this dataset have different scales of magnitude, hence, before constructing and training a neural network model, we normalize the dataset. For this, we use the MinMaxScaler API from sklearn, which normalizes each column's values between 0 and 1, as discussed in the Logistic Regression section of this chapter (see Exercise 5.03, Logistic Regression – Multiclass Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, y_orig = df.loc[:,'mean radius':'worst fractal dimension'], df.loc[:,'diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0       0.521037      0.022658        0.545989   0.363733         0.593753   \n1       0.643144      0.272574        0.615783   0.501591         0.289880   \n2       0.601496      0.390260        0.595743   0.449417         0.514309   \n3       0.210090      0.360839        0.233501   0.102906         0.811321   \n4       0.629893      0.156578        0.630986   0.489290         0.430351   \n..           ...           ...             ...        ...              ...   \n564     0.690000      0.428813        0.678668   0.566490         0.526948   \n565     0.622320      0.626987        0.604036   0.474019         0.407782   \n566     0.455251      0.621238        0.445788   0.303118         0.288165   \n567     0.644564      0.663510        0.665538   0.475716         0.588336   \n568     0.036869      0.501522        0.028540   0.015907         0.000000   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0            0.792037        0.703140             0.731113       0.686364   \n1            0.181768        0.203608             0.348757       0.379798   \n2            0.431017        0.462512             0.635686       0.509596   \n3            0.811361        0.565604             0.522863       0.776263   \n4            0.347893        0.463918             0.518390       0.378283   \n..                ...             ...                  ...            ...   \n564          0.296055        0.571462             0.690358       0.336364   \n565          0.257714        0.337395             0.486630       0.349495   \n566          0.254340        0.216753             0.263519       0.267677   \n567          0.790197        0.823336             0.755467       0.675253   \n568          0.074351        0.000000             0.000000       0.266162   \n\n     mean fractal dimension  ...  worst radius  worst texture  \\\n0                  0.605518  ...      0.620776       0.141525   \n1                  0.141323  ...      0.606901       0.303571   \n2                  0.211247  ...      0.556386       0.360075   \n3                  1.000000  ...      0.248310       0.385928   \n4                  0.186816  ...      0.519744       0.123934   \n..                      ...  ...           ...            ...   \n564                0.132056  ...      0.623266       0.383262   \n565                0.113100  ...      0.560655       0.699094   \n566                0.137321  ...      0.393099       0.589019   \n567                0.425442  ...      0.633582       0.730277   \n568                0.187026  ...      0.054287       0.489072   \n\n     worst perimeter  worst area  worst smoothness  worst compactness  \\\n0           0.668310    0.450698          0.601136           0.619292   \n1           0.539818    0.435214          0.347553           0.154563   \n2           0.508442    0.374508          0.483590           0.385375   \n3           0.241347    0.094008          0.915472           0.814012   \n4           0.506948    0.341575          0.437364           0.172415   \n..               ...         ...               ...                ...   \n564         0.576174    0.452664          0.461137           0.178527   \n565         0.520892    0.379915          0.300007           0.159997   \n566         0.379949    0.230731          0.282177           0.273705   \n567         0.668310    0.402035          0.619626           0.815758   \n568         0.043578    0.020497          0.124084           0.036043   \n\n     worst concavity  worst concave points  worst symmetry  \\\n0           0.568610              0.912027        0.598462   \n1           0.192971              0.639175        0.233590   \n2           0.359744              0.835052        0.403706   \n3           0.548642              0.884880        1.000000   \n4           0.319489              0.558419        0.157500   \n..               ...                   ...             ...   \n564         0.328035              0.761512        0.097575   \n565         0.256789              0.559450        0.198502   \n566         0.271805              0.487285        0.128721   \n567         0.749760              0.910653        0.497142   \n568         0.000000              0.000000        0.257441   \n\n     worst fractal dimension  \n0                   0.418864  \n1                   0.222878  \n2                   0.213433  \n3                   0.773711  \n4                   0.142595  \n..                       ...  \n564                 0.105667  \n565                 0.074315  \n566                 0.151909  \n567                 0.452315  \n568                 0.100682  \n\n[569 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.521037</td>\n      <td>0.022658</td>\n      <td>0.545989</td>\n      <td>0.363733</td>\n      <td>0.593753</td>\n      <td>0.792037</td>\n      <td>0.703140</td>\n      <td>0.731113</td>\n      <td>0.686364</td>\n      <td>0.605518</td>\n      <td>...</td>\n      <td>0.620776</td>\n      <td>0.141525</td>\n      <td>0.668310</td>\n      <td>0.450698</td>\n      <td>0.601136</td>\n      <td>0.619292</td>\n      <td>0.568610</td>\n      <td>0.912027</td>\n      <td>0.598462</td>\n      <td>0.418864</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.643144</td>\n      <td>0.272574</td>\n      <td>0.615783</td>\n      <td>0.501591</td>\n      <td>0.289880</td>\n      <td>0.181768</td>\n      <td>0.203608</td>\n      <td>0.348757</td>\n      <td>0.379798</td>\n      <td>0.141323</td>\n      <td>...</td>\n      <td>0.606901</td>\n      <td>0.303571</td>\n      <td>0.539818</td>\n      <td>0.435214</td>\n      <td>0.347553</td>\n      <td>0.154563</td>\n      <td>0.192971</td>\n      <td>0.639175</td>\n      <td>0.233590</td>\n      <td>0.222878</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.601496</td>\n      <td>0.390260</td>\n      <td>0.595743</td>\n      <td>0.449417</td>\n      <td>0.514309</td>\n      <td>0.431017</td>\n      <td>0.462512</td>\n      <td>0.635686</td>\n      <td>0.509596</td>\n      <td>0.211247</td>\n      <td>...</td>\n      <td>0.556386</td>\n      <td>0.360075</td>\n      <td>0.508442</td>\n      <td>0.374508</td>\n      <td>0.483590</td>\n      <td>0.385375</td>\n      <td>0.359744</td>\n      <td>0.835052</td>\n      <td>0.403706</td>\n      <td>0.213433</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.210090</td>\n      <td>0.360839</td>\n      <td>0.233501</td>\n      <td>0.102906</td>\n      <td>0.811321</td>\n      <td>0.811361</td>\n      <td>0.565604</td>\n      <td>0.522863</td>\n      <td>0.776263</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.248310</td>\n      <td>0.385928</td>\n      <td>0.241347</td>\n      <td>0.094008</td>\n      <td>0.915472</td>\n      <td>0.814012</td>\n      <td>0.548642</td>\n      <td>0.884880</td>\n      <td>1.000000</td>\n      <td>0.773711</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.629893</td>\n      <td>0.156578</td>\n      <td>0.630986</td>\n      <td>0.489290</td>\n      <td>0.430351</td>\n      <td>0.347893</td>\n      <td>0.463918</td>\n      <td>0.518390</td>\n      <td>0.378283</td>\n      <td>0.186816</td>\n      <td>...</td>\n      <td>0.519744</td>\n      <td>0.123934</td>\n      <td>0.506948</td>\n      <td>0.341575</td>\n      <td>0.437364</td>\n      <td>0.172415</td>\n      <td>0.319489</td>\n      <td>0.558419</td>\n      <td>0.157500</td>\n      <td>0.142595</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>0.690000</td>\n      <td>0.428813</td>\n      <td>0.678668</td>\n      <td>0.566490</td>\n      <td>0.526948</td>\n      <td>0.296055</td>\n      <td>0.571462</td>\n      <td>0.690358</td>\n      <td>0.336364</td>\n      <td>0.132056</td>\n      <td>...</td>\n      <td>0.623266</td>\n      <td>0.383262</td>\n      <td>0.576174</td>\n      <td>0.452664</td>\n      <td>0.461137</td>\n      <td>0.178527</td>\n      <td>0.328035</td>\n      <td>0.761512</td>\n      <td>0.097575</td>\n      <td>0.105667</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>0.622320</td>\n      <td>0.626987</td>\n      <td>0.604036</td>\n      <td>0.474019</td>\n      <td>0.407782</td>\n      <td>0.257714</td>\n      <td>0.337395</td>\n      <td>0.486630</td>\n      <td>0.349495</td>\n      <td>0.113100</td>\n      <td>...</td>\n      <td>0.560655</td>\n      <td>0.699094</td>\n      <td>0.520892</td>\n      <td>0.379915</td>\n      <td>0.300007</td>\n      <td>0.159997</td>\n      <td>0.256789</td>\n      <td>0.559450</td>\n      <td>0.198502</td>\n      <td>0.074315</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>0.455251</td>\n      <td>0.621238</td>\n      <td>0.445788</td>\n      <td>0.303118</td>\n      <td>0.288165</td>\n      <td>0.254340</td>\n      <td>0.216753</td>\n      <td>0.263519</td>\n      <td>0.267677</td>\n      <td>0.137321</td>\n      <td>...</td>\n      <td>0.393099</td>\n      <td>0.589019</td>\n      <td>0.379949</td>\n      <td>0.230731</td>\n      <td>0.282177</td>\n      <td>0.273705</td>\n      <td>0.271805</td>\n      <td>0.487285</td>\n      <td>0.128721</td>\n      <td>0.151909</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>0.644564</td>\n      <td>0.663510</td>\n      <td>0.665538</td>\n      <td>0.475716</td>\n      <td>0.588336</td>\n      <td>0.790197</td>\n      <td>0.823336</td>\n      <td>0.755467</td>\n      <td>0.675253</td>\n      <td>0.425442</td>\n      <td>...</td>\n      <td>0.633582</td>\n      <td>0.730277</td>\n      <td>0.668310</td>\n      <td>0.402035</td>\n      <td>0.619626</td>\n      <td>0.815758</td>\n      <td>0.749760</td>\n      <td>0.910653</td>\n      <td>0.497142</td>\n      <td>0.452315</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>0.036869</td>\n      <td>0.501522</td>\n      <td>0.028540</td>\n      <td>0.015907</td>\n      <td>0.000000</td>\n      <td>0.074351</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.266162</td>\n      <td>0.187026</td>\n      <td>...</td>\n      <td>0.054287</td>\n      <td>0.489072</td>\n      <td>0.043578</td>\n      <td>0.020497</td>\n      <td>0.124084</td>\n      <td>0.036043</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.257441</td>\n      <td>0.100682</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X_orig)\n",
    "X = pd.DataFrame(scaled, columns=X_orig.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can construct the model, we must first convert the diagnosis values into labels that can be used within the model. Replace the benign diagnosis string with the value 0, and the malignant diagnosis string with the value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n564    1\n565    1\n566    1\n567    1\n568    0\nName: diagnosis, Length: 569, dtype: int64"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "diag = {\n",
    "    'benign': 0,\n",
    "    'malignant' : 1\n",
    "}\n",
    "y = y_orig.replace(diag)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, in order to impartially evaluate the model, we should split the training dataset into a training and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n190     0.342610      0.453500        0.349527   0.197837         0.495351   \n134     0.542808      0.412580        0.528022   0.395122         0.376185   \n386     0.247480      0.148123        0.241794   0.135101         0.256838   \n118     0.416442      0.446398        0.427821   0.271092         0.567572   \n316     0.246060      0.147785        0.231221   0.134846         0.223075   \n..           ...           ...             ...        ...              ...   \n98      0.218609      0.105851        0.211112   0.114146         0.335831   \n322     0.278243      0.122083        0.269712   0.153256         0.548614   \n382     0.239907      0.439973        0.241587   0.129077         0.150943   \n365     0.636992      0.408184        0.622003   0.487593         0.350907   \n510     0.225235      0.168414        0.224725   0.119830         0.256026   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n190          0.680694        0.464152             0.328926       0.668687   \n134          0.238360        0.270150             0.340308       0.319192   \n386          0.180510        0.160239             0.125944       0.295960   \n118          0.477946        0.499766             0.471123       0.523232   \n316          0.039077        0.026312             0.025104       0.309596   \n..                ...             ...                  ...            ...   \n98           0.171370        0.098313             0.166501       0.282828   \n322          0.211521        0.089035             0.168986       0.243939   \n382          0.269677        0.186106             0.148012       0.072222   \n365          0.287467        0.229592             0.386928       0.281818   \n510          0.236887        0.157591             0.131163       0.221717   \n\n     mean fractal dimension  ...  worst radius  worst texture  \\\n190                0.536226  ...      0.277837       0.670576   \n134                0.153960  ...      0.519032       0.516258   \n386                0.243892  ...      0.184988       0.193763   \n118                0.491786  ...      0.436144       0.492537   \n316                0.137532  ...      0.175027       0.118603   \n..                      ...  ...           ...            ...   \n98                 0.334035  ...      0.182497       0.136994   \n322                0.311710  ...      0.217360       0.241471   \n382                0.350253  ...      0.165066       0.444829   \n365                0.118155  ...      0.582711       0.382463   \n510                0.371104  ...      0.160797       0.148721   \n\n     worst perimeter  worst area  worst smoothness  worst compactness  \\\n190         0.278849    0.141860          0.542363           0.878433   \n134         0.474077    0.345262          0.497458           0.194245   \n386         0.185467    0.084718          0.207555           0.209380   \n118         0.397878    0.267106          0.755002           0.451349   \n316         0.155336    0.080589          0.191045           0.025254   \n..               ...         ...               ...                ...   \n98          0.162110    0.080441          0.475005           0.153108   \n322         0.211116    0.101824          0.551608           0.189976   \n382         0.184023    0.074518          0.111074           0.285347   \n365         0.551771    0.391958          0.406326           0.204044   \n510         0.153593    0.070930          0.238592           0.244501   \n\n     worst concavity  worst concave points  worst symmetry  \\\n190         0.677955              0.608935        0.709836   \n134         0.316693              0.473883        0.304356   \n386         0.245687              0.314089        0.219200   \n118         0.587540              0.698969        0.336882   \n316         0.032875              0.063643        0.143505   \n..               ...                   ...             ...   \n98          0.153514              0.290344        0.237926   \n322         0.143051              0.396907        0.161049   \n382         0.232588              0.375258        0.123398   \n365         0.215815              0.606529        0.205795   \n510         0.214856              0.362887        0.204810   \n\n     worst fractal dimension  \n190                 0.587433  \n134                 0.138135  \n386                 0.217762  \n118                 0.460186  \n316                 0.034960  \n..                       ...  \n98                  0.213302  \n322                 0.199987  \n382                 0.252197  \n365                 0.080742  \n510                 0.286961  \n\n[455 rows x 30 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>190</th>\n      <td>0.342610</td>\n      <td>0.453500</td>\n      <td>0.349527</td>\n      <td>0.197837</td>\n      <td>0.495351</td>\n      <td>0.680694</td>\n      <td>0.464152</td>\n      <td>0.328926</td>\n      <td>0.668687</td>\n      <td>0.536226</td>\n      <td>...</td>\n      <td>0.277837</td>\n      <td>0.670576</td>\n      <td>0.278849</td>\n      <td>0.141860</td>\n      <td>0.542363</td>\n      <td>0.878433</td>\n      <td>0.677955</td>\n      <td>0.608935</td>\n      <td>0.709836</td>\n      <td>0.587433</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>0.542808</td>\n      <td>0.412580</td>\n      <td>0.528022</td>\n      <td>0.395122</td>\n      <td>0.376185</td>\n      <td>0.238360</td>\n      <td>0.270150</td>\n      <td>0.340308</td>\n      <td>0.319192</td>\n      <td>0.153960</td>\n      <td>...</td>\n      <td>0.519032</td>\n      <td>0.516258</td>\n      <td>0.474077</td>\n      <td>0.345262</td>\n      <td>0.497458</td>\n      <td>0.194245</td>\n      <td>0.316693</td>\n      <td>0.473883</td>\n      <td>0.304356</td>\n      <td>0.138135</td>\n    </tr>\n    <tr>\n      <th>386</th>\n      <td>0.247480</td>\n      <td>0.148123</td>\n      <td>0.241794</td>\n      <td>0.135101</td>\n      <td>0.256838</td>\n      <td>0.180510</td>\n      <td>0.160239</td>\n      <td>0.125944</td>\n      <td>0.295960</td>\n      <td>0.243892</td>\n      <td>...</td>\n      <td>0.184988</td>\n      <td>0.193763</td>\n      <td>0.185467</td>\n      <td>0.084718</td>\n      <td>0.207555</td>\n      <td>0.209380</td>\n      <td>0.245687</td>\n      <td>0.314089</td>\n      <td>0.219200</td>\n      <td>0.217762</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>0.416442</td>\n      <td>0.446398</td>\n      <td>0.427821</td>\n      <td>0.271092</td>\n      <td>0.567572</td>\n      <td>0.477946</td>\n      <td>0.499766</td>\n      <td>0.471123</td>\n      <td>0.523232</td>\n      <td>0.491786</td>\n      <td>...</td>\n      <td>0.436144</td>\n      <td>0.492537</td>\n      <td>0.397878</td>\n      <td>0.267106</td>\n      <td>0.755002</td>\n      <td>0.451349</td>\n      <td>0.587540</td>\n      <td>0.698969</td>\n      <td>0.336882</td>\n      <td>0.460186</td>\n    </tr>\n    <tr>\n      <th>316</th>\n      <td>0.246060</td>\n      <td>0.147785</td>\n      <td>0.231221</td>\n      <td>0.134846</td>\n      <td>0.223075</td>\n      <td>0.039077</td>\n      <td>0.026312</td>\n      <td>0.025104</td>\n      <td>0.309596</td>\n      <td>0.137532</td>\n      <td>...</td>\n      <td>0.175027</td>\n      <td>0.118603</td>\n      <td>0.155336</td>\n      <td>0.080589</td>\n      <td>0.191045</td>\n      <td>0.025254</td>\n      <td>0.032875</td>\n      <td>0.063643</td>\n      <td>0.143505</td>\n      <td>0.034960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>0.218609</td>\n      <td>0.105851</td>\n      <td>0.211112</td>\n      <td>0.114146</td>\n      <td>0.335831</td>\n      <td>0.171370</td>\n      <td>0.098313</td>\n      <td>0.166501</td>\n      <td>0.282828</td>\n      <td>0.334035</td>\n      <td>...</td>\n      <td>0.182497</td>\n      <td>0.136994</td>\n      <td>0.162110</td>\n      <td>0.080441</td>\n      <td>0.475005</td>\n      <td>0.153108</td>\n      <td>0.153514</td>\n      <td>0.290344</td>\n      <td>0.237926</td>\n      <td>0.213302</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>0.278243</td>\n      <td>0.122083</td>\n      <td>0.269712</td>\n      <td>0.153256</td>\n      <td>0.548614</td>\n      <td>0.211521</td>\n      <td>0.089035</td>\n      <td>0.168986</td>\n      <td>0.243939</td>\n      <td>0.311710</td>\n      <td>...</td>\n      <td>0.217360</td>\n      <td>0.241471</td>\n      <td>0.211116</td>\n      <td>0.101824</td>\n      <td>0.551608</td>\n      <td>0.189976</td>\n      <td>0.143051</td>\n      <td>0.396907</td>\n      <td>0.161049</td>\n      <td>0.199987</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>0.239907</td>\n      <td>0.439973</td>\n      <td>0.241587</td>\n      <td>0.129077</td>\n      <td>0.150943</td>\n      <td>0.269677</td>\n      <td>0.186106</td>\n      <td>0.148012</td>\n      <td>0.072222</td>\n      <td>0.350253</td>\n      <td>...</td>\n      <td>0.165066</td>\n      <td>0.444829</td>\n      <td>0.184023</td>\n      <td>0.074518</td>\n      <td>0.111074</td>\n      <td>0.285347</td>\n      <td>0.232588</td>\n      <td>0.375258</td>\n      <td>0.123398</td>\n      <td>0.252197</td>\n    </tr>\n    <tr>\n      <th>365</th>\n      <td>0.636992</td>\n      <td>0.408184</td>\n      <td>0.622003</td>\n      <td>0.487593</td>\n      <td>0.350907</td>\n      <td>0.287467</td>\n      <td>0.229592</td>\n      <td>0.386928</td>\n      <td>0.281818</td>\n      <td>0.118155</td>\n      <td>...</td>\n      <td>0.582711</td>\n      <td>0.382463</td>\n      <td>0.551771</td>\n      <td>0.391958</td>\n      <td>0.406326</td>\n      <td>0.204044</td>\n      <td>0.215815</td>\n      <td>0.606529</td>\n      <td>0.205795</td>\n      <td>0.080742</td>\n    </tr>\n    <tr>\n      <th>510</th>\n      <td>0.225235</td>\n      <td>0.168414</td>\n      <td>0.224725</td>\n      <td>0.119830</td>\n      <td>0.256026</td>\n      <td>0.236887</td>\n      <td>0.157591</td>\n      <td>0.131163</td>\n      <td>0.221717</td>\n      <td>0.371104</td>\n      <td>...</td>\n      <td>0.160797</td>\n      <td>0.148721</td>\n      <td>0.153593</td>\n      <td>0.070930</td>\n      <td>0.238592</td>\n      <td>0.244501</td>\n      <td>0.214856</td>\n      <td>0.362887</td>\n      <td>0.204810</td>\n      <td>0.286961</td>\n    </tr>\n  </tbody>\n</table>\n<p>455 rows × 30 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.2,\n",
    "    random_state=123\n",
    ")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model using the normalized dataset and the assigned diagnosis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(100,), learning_rate='constant',\n              learning_rate_init=0.01, max_fun=15000, max_iter=1000,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=1, shuffle=True, solver='sgd',\n              tol=0.0001, validation_fraction=0.1, verbose=False,\n              warm_start=False)"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "model1 = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    solver='sgd',\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=1000,\n",
    "    random_state=1\n",
    ")\n",
    "model1.fit(\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of the model against the validation set. The output will be similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9824561403508771"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model1.score(\n",
    "    X=X_test,\n",
    "    y=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9912280701754386"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "model2 = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,), # új layer vagy neuronok számának növelése nem hozott javulást\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.01, # ha ezt leveszem 0.001-re, nem változtat a score-on\n",
    "    max_iter=1000, # # ha ezt növelem 5000-re, nem változtat a score-on\n",
    "    random_state=1\n",
    ")\n",
    "model2.fit(\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")\n",
    "model2.score(\n",
    "    X=X_test,\n",
    "    y=y_test\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "slw",
   "display_name": "slw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}