{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600712959452",
   "display_name": "Python 3.7.2 64-bit ('dsw': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Balancing Techniques on a Telecom Dataset\n",
    "\n",
    "Now that we have seen different balancing techniques, let's apply these techniques to a new dataset that is related to the churn of telecom customers. This dataset is available on GitHub.\n",
    "\n",
    "This dataset has various variables related to the usage level of a mobile connection, such as total call minutes, call charges, calls made during certain periods of the day, details of international calls, and details of calls to customer services.\n",
    "\n",
    "The problem statement is to predict whether a customer will churn. This dataset is a highly imbalanced one, with the cases where customers churn being the minority. You will be using this dataset in the following activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Best Balancing Technique by Fitting a Classifier on the Telecom Churn Dataset\n",
    "\n",
    "You are working as a data scientist for a telecom company. You have encountered a dataset that is highly imbalanced, and you want to correct the class imbalance before fitting the classifier to analyze the churn. You know different methods for correcting the imbalance in datasets and you want to compare them to find the best method before fitting the model.\n",
    "\n",
    "In this activity, you need to implement all of the three methods that you have come across so far and compare the results.\n",
    "\n",
    "Note: You will be using the telecom churn dataset that you used in Chapter 10, Analyzing a Dataset.\n",
    "\n",
    "Use the MinMaxscaler function to scale the dataset instead of the robust scaler function you have been using so far. Compare the methods based on the results you get by fitting a logistic regression model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import smote_variants as sv \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     churn  accountlength  ... totalintlcharge numbercustomerservicecalls\n0       No            128  ...            2.70                          1\n1       No            107  ...            3.70                          1\n2       No            137  ...            3.29                          0\n3       No             84  ...            1.78                          2\n4       No             75  ...            2.73                          3\n...    ...            ...  ...             ...                        ...\n4995    No             50  ...            2.67                          2\n4996   Yes            152  ...            3.97                          3\n4997    No             61  ...            3.67                          1\n4998    No            109  ...            2.30                          0\n4999    No             86  ...            2.51                          0\n\n[5000 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>churn</th>\n      <th>accountlength</th>\n      <th>internationalplan</th>\n      <th>voicemailplan</th>\n      <th>numbervmailmessages</th>\n      <th>totaldayminutes</th>\n      <th>totaldaycalls</th>\n      <th>totaldaycharge</th>\n      <th>totaleveminutes</th>\n      <th>totalevecalls</th>\n      <th>totalevecharge</th>\n      <th>totalnightminutes</th>\n      <th>totalnightcalls</th>\n      <th>totalnightcharge</th>\n      <th>totalintlminutes</th>\n      <th>totalintlcalls</th>\n      <th>totalintlcharge</th>\n      <th>numbercustomerservicecalls</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>No</td>\n      <td>128</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>25</td>\n      <td>265.1</td>\n      <td>110</td>\n      <td>45.07</td>\n      <td>197.4</td>\n      <td>99</td>\n      <td>16.78</td>\n      <td>244.7</td>\n      <td>91</td>\n      <td>11.01</td>\n      <td>10.0</td>\n      <td>3</td>\n      <td>2.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>No</td>\n      <td>107</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>26</td>\n      <td>161.6</td>\n      <td>123</td>\n      <td>27.47</td>\n      <td>195.5</td>\n      <td>103</td>\n      <td>16.62</td>\n      <td>254.4</td>\n      <td>103</td>\n      <td>11.45</td>\n      <td>13.7</td>\n      <td>3</td>\n      <td>3.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>137</td>\n      <td>no</td>\n      <td>no</td>\n      <td>0</td>\n      <td>243.4</td>\n      <td>114</td>\n      <td>41.38</td>\n      <td>121.2</td>\n      <td>110</td>\n      <td>10.30</td>\n      <td>162.6</td>\n      <td>104</td>\n      <td>7.32</td>\n      <td>12.2</td>\n      <td>5</td>\n      <td>3.29</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>84</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>0</td>\n      <td>299.4</td>\n      <td>71</td>\n      <td>50.90</td>\n      <td>61.9</td>\n      <td>88</td>\n      <td>5.26</td>\n      <td>196.9</td>\n      <td>89</td>\n      <td>8.86</td>\n      <td>6.6</td>\n      <td>7</td>\n      <td>1.78</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>No</td>\n      <td>75</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>0</td>\n      <td>166.7</td>\n      <td>113</td>\n      <td>28.34</td>\n      <td>148.3</td>\n      <td>122</td>\n      <td>12.61</td>\n      <td>186.9</td>\n      <td>121</td>\n      <td>8.41</td>\n      <td>10.1</td>\n      <td>3</td>\n      <td>2.73</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>No</td>\n      <td>50</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>40</td>\n      <td>235.7</td>\n      <td>127</td>\n      <td>40.07</td>\n      <td>223.0</td>\n      <td>126</td>\n      <td>18.96</td>\n      <td>297.5</td>\n      <td>116</td>\n      <td>13.39</td>\n      <td>9.9</td>\n      <td>5</td>\n      <td>2.67</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>Yes</td>\n      <td>152</td>\n      <td>no</td>\n      <td>no</td>\n      <td>0</td>\n      <td>184.2</td>\n      <td>90</td>\n      <td>31.31</td>\n      <td>256.8</td>\n      <td>73</td>\n      <td>21.83</td>\n      <td>213.6</td>\n      <td>113</td>\n      <td>9.61</td>\n      <td>14.7</td>\n      <td>2</td>\n      <td>3.97</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>No</td>\n      <td>61</td>\n      <td>no</td>\n      <td>no</td>\n      <td>0</td>\n      <td>140.6</td>\n      <td>89</td>\n      <td>23.90</td>\n      <td>172.8</td>\n      <td>128</td>\n      <td>14.69</td>\n      <td>212.4</td>\n      <td>97</td>\n      <td>9.56</td>\n      <td>13.6</td>\n      <td>4</td>\n      <td>3.67</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>No</td>\n      <td>109</td>\n      <td>no</td>\n      <td>no</td>\n      <td>0</td>\n      <td>188.8</td>\n      <td>67</td>\n      <td>32.10</td>\n      <td>171.7</td>\n      <td>92</td>\n      <td>14.59</td>\n      <td>224.4</td>\n      <td>89</td>\n      <td>10.10</td>\n      <td>8.5</td>\n      <td>6</td>\n      <td>2.30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>No</td>\n      <td>86</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>34</td>\n      <td>129.4</td>\n      <td>102</td>\n      <td>22.00</td>\n      <td>267.1</td>\n      <td>104</td>\n      <td>22.70</td>\n      <td>154.8</td>\n      <td>100</td>\n      <td>6.97</td>\n      <td>9.3</td>\n      <td>16</td>\n      <td>2.51</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 18 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv('../Dataset/churn.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      churn  accountlength  ...  totalintlcharge  numbercustomerservicecalls\n0         0            128  ...             2.70                           1\n1         0            107  ...             3.70                           1\n2         0            137  ...             3.29                           0\n3         0             84  ...             1.78                           2\n4         0             75  ...             2.73                           3\n...     ...            ...  ...              ...                         ...\n4995      0             50  ...             2.67                           2\n4996      1            152  ...             3.97                           3\n4997      0             61  ...             3.67                           1\n4998      0            109  ...             2.30                           0\n4999      0             86  ...             2.51                           0\n\n[5000 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>churn</th>\n      <th>accountlength</th>\n      <th>internationalplan</th>\n      <th>voicemailplan</th>\n      <th>numbervmailmessages</th>\n      <th>totaldayminutes</th>\n      <th>totaldaycalls</th>\n      <th>totaldaycharge</th>\n      <th>totaleveminutes</th>\n      <th>totalevecalls</th>\n      <th>totalevecharge</th>\n      <th>totalnightminutes</th>\n      <th>totalnightcalls</th>\n      <th>totalnightcharge</th>\n      <th>totalintlminutes</th>\n      <th>totalintlcalls</th>\n      <th>totalintlcharge</th>\n      <th>numbercustomerservicecalls</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>128</td>\n      <td>0</td>\n      <td>1</td>\n      <td>25</td>\n      <td>265.1</td>\n      <td>110</td>\n      <td>45.07</td>\n      <td>197.4</td>\n      <td>99</td>\n      <td>16.78</td>\n      <td>244.7</td>\n      <td>91</td>\n      <td>11.01</td>\n      <td>10.0</td>\n      <td>3</td>\n      <td>2.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>107</td>\n      <td>0</td>\n      <td>1</td>\n      <td>26</td>\n      <td>161.6</td>\n      <td>123</td>\n      <td>27.47</td>\n      <td>195.5</td>\n      <td>103</td>\n      <td>16.62</td>\n      <td>254.4</td>\n      <td>103</td>\n      <td>11.45</td>\n      <td>13.7</td>\n      <td>3</td>\n      <td>3.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>137</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>243.4</td>\n      <td>114</td>\n      <td>41.38</td>\n      <td>121.2</td>\n      <td>110</td>\n      <td>10.30</td>\n      <td>162.6</td>\n      <td>104</td>\n      <td>7.32</td>\n      <td>12.2</td>\n      <td>5</td>\n      <td>3.29</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>84</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>299.4</td>\n      <td>71</td>\n      <td>50.90</td>\n      <td>61.9</td>\n      <td>88</td>\n      <td>5.26</td>\n      <td>196.9</td>\n      <td>89</td>\n      <td>8.86</td>\n      <td>6.6</td>\n      <td>7</td>\n      <td>1.78</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>75</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>166.7</td>\n      <td>113</td>\n      <td>28.34</td>\n      <td>148.3</td>\n      <td>122</td>\n      <td>12.61</td>\n      <td>186.9</td>\n      <td>121</td>\n      <td>8.41</td>\n      <td>10.1</td>\n      <td>3</td>\n      <td>2.73</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>0</td>\n      <td>50</td>\n      <td>0</td>\n      <td>1</td>\n      <td>40</td>\n      <td>235.7</td>\n      <td>127</td>\n      <td>40.07</td>\n      <td>223.0</td>\n      <td>126</td>\n      <td>18.96</td>\n      <td>297.5</td>\n      <td>116</td>\n      <td>13.39</td>\n      <td>9.9</td>\n      <td>5</td>\n      <td>2.67</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>1</td>\n      <td>152</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>184.2</td>\n      <td>90</td>\n      <td>31.31</td>\n      <td>256.8</td>\n      <td>73</td>\n      <td>21.83</td>\n      <td>213.6</td>\n      <td>113</td>\n      <td>9.61</td>\n      <td>14.7</td>\n      <td>2</td>\n      <td>3.97</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>0</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>140.6</td>\n      <td>89</td>\n      <td>23.90</td>\n      <td>172.8</td>\n      <td>128</td>\n      <td>14.69</td>\n      <td>212.4</td>\n      <td>97</td>\n      <td>9.56</td>\n      <td>13.6</td>\n      <td>4</td>\n      <td>3.67</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>0</td>\n      <td>109</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>188.8</td>\n      <td>67</td>\n      <td>32.10</td>\n      <td>171.7</td>\n      <td>92</td>\n      <td>14.59</td>\n      <td>224.4</td>\n      <td>89</td>\n      <td>10.10</td>\n      <td>8.5</td>\n      <td>6</td>\n      <td>2.30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>0</td>\n      <td>86</td>\n      <td>0</td>\n      <td>1</td>\n      <td>34</td>\n      <td>129.4</td>\n      <td>102</td>\n      <td>22.00</td>\n      <td>267.1</td>\n      <td>104</td>\n      <td>22.70</td>\n      <td>154.8</td>\n      <td>100</td>\n      <td>6.97</td>\n      <td>9.3</td>\n      <td>16</td>\n      <td>2.51</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 18 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df['internationalplan'] = df['internationalplan'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "df['voicemailplan'] = df['voicemailplan'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "df['churn'] = df['churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4000, 17)\n(1000, 17)\n(4000,)\n(1000,)\n"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "553\n3447\n"
    }
   ],
   "source": [
    "# check imbalance\n",
    "print(y_train.churn[y_train.churn==1].count())\n",
    "print(y_train.churn[y_train.churn==0].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(553, 17)\n(553, 1)\n"
    }
   ],
   "source": [
    "# minority class\n",
    "ndx_min = y_train[y_train.churn==1].index\n",
    "X_train_min = X_train.iloc[ndx_min, :]\n",
    "y_train_min = y_train.iloc[ndx_min, :]\n",
    "print(X_train_min.shape)\n",
    "print(y_train_min.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3447, 17)\n(3447, 1)\n"
    }
   ],
   "source": [
    "# majority class\n",
    "ndx_maj = y_train[y_train.churn==0].index\n",
    "X_train_maj = X_train.iloc[ndx_maj, :]\n",
    "y_train_maj = y_train.iloc[ndx_maj, :]\n",
    "print(X_train_maj.shape)\n",
    "print(y_train_maj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(553, 17)\n(553, 1)\n"
    }
   ],
   "source": [
    "# sample majority class\n",
    "X_train_maj_sample = X_train_maj.sample(n=len(ndx_min),random_state = 123)\n",
    "y_train_maj_sample = y_train.iloc[X_train_maj_sample.index, :]\n",
    "print(X_train_maj_sample.shape)\n",
    "print(y_train_maj_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1106, 17)\n(1106,)\n"
    }
   ],
   "source": [
    "# concat\n",
    "X_train_us = pd.concat([X_train_min, X_train_maj_sample], axis=0)\n",
    "y_train_us = pd.concat([y_train_min, y_train_maj_sample], axis=0)\n",
    "# shuffle\n",
    "temp_data = pd.concat([X_train_us, y_train_us], axis=1)\n",
    "temp_data = shuffle(temp_data)\n",
    "# re_separate\n",
    "X_train_us = temp_data.iloc[:, :-1]\n",
    "y_train_us = temp_data.iloc[:, -1]\n",
    "\n",
    "print(X_train_us.shape)\n",
    "print(y_train_us.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnModel1 = LogisticRegression()\n",
    "churnModel1.fit(X_train_us, y_train_us)\n",
    "y_pred1 = churnModel1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-21 21:24:58,793:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 123}\")\n(6894, 17)\n(6894,)\n"
    }
   ],
   "source": [
    "oversampler_smote = sv.SMOTE(random_state=123)\n",
    "X_train_smote, y_train_smote = oversampler_smote.sample(X_train.values, y_train.values.flatten())\n",
    "print(X_train_smote.shape)\n",
    "print(y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3447\n3447\n"
    }
   ],
   "source": [
    "print(sum(y_train_smote==1))\n",
    "print(sum(y_train_smote==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnModel2 = LogisticRegression()\n",
    "churnModel2.fit(X_train_smote, y_train_smote)\n",
    "y_pred2 = churnModel2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-21 21:32:47,574:INFO:MSMOTE: Running sampling via ('MSMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 123}\")\n(6894, 17)\n(6894,)\n"
    }
   ],
   "source": [
    "oversampler_msmote = sv.MSMOTE(random_state=123)\n",
    "X_train_msmote, y_train_msmote = oversampler_msmote.sample(X_train.values, y_train.values.flatten())\n",
    "print(X_train_msmote.shape)\n",
    "print(y_train_msmote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3447\n3447\n"
    }
   ],
   "source": [
    "print(sum(y_train_msmote==1))\n",
    "print(sum(y_train_msmote==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnModel3 = LogisticRegression()\n",
    "churnModel3.fit(X_train_msmote, y_train_msmote)\n",
    "y_pred3 = churnModel3.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ChurnModel1\nAccuracy: 0.799\n[[674 172]\n [ 29 125]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.80      0.87       846\n           1       0.42      0.81      0.55       154\n\n    accuracy                           0.80      1000\n   macro avg       0.69      0.80      0.71      1000\nweighted avg       0.88      0.80      0.82      1000\n\n\nChurnModel2\nAccuracy: 0.794\n[[672 174]\n [ 32 122]]\n              precision    recall  f1-score   support\n\n           0       0.95      0.79      0.87       846\n           1       0.41      0.79      0.54       154\n\n    accuracy                           0.79      1000\n   macro avg       0.68      0.79      0.70      1000\nweighted avg       0.87      0.79      0.82      1000\n\n\nChurnModel3\nAccuracy: 0.802\n[[682 164]\n [ 34 120]]\n              precision    recall  f1-score   support\n\n           0       0.95      0.81      0.87       846\n           1       0.42      0.78      0.55       154\n\n    accuracy                           0.80      1000\n   macro avg       0.69      0.79      0.71      1000\nweighted avg       0.87      0.80      0.82      1000\n\n"
    }
   ],
   "source": [
    "print('ChurnModel1')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred1)}')\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "print('\\nChurnModel2')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred2)}')\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "\n",
    "\n",
    "print('\\nChurnModel3')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred3)}')\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we have performed data balancing using random undersampling with SMOTE and MSMOTE for the telecom churn dataset. From the classification report, we can see that MSMOTE has the best accuracy, 80%, compared to SMOTE and undersampling techniques, which achieve 79% and 78%, respectively. However, we know that it is important to look at the recall values, especially of the minority class. From the recall values, we see that SMOTE has the largest value of 76%. This means that 76% of customers who are likely to churn have been correctly identified by the model. Random undersampling and MSMOTE have lower recall values of 73% and 75%, respectively. We now have a situation where MSMOTE has the highest accuracy but a slightly lower recall value and SMOTE has the lowest accuracy measure but the highest recall value. In such a situation, we have to look at the f1 scores, which is a weighted score between precision and recall. From all the f1 scores, we see that MSMOTE has the highest f1 score of 52%, with SMOTE and random undersampling scoring 50% each. Therefore, we can select MSMOTE as the best technique for balancing for this context."
   ]
  }
 ]
}