{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597741783416",
   "display_name": "Python 3.7.2 64-bit ('dsw': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find an Optimal Model for Predicting the Critical Temperatures of Superconductors\n",
    "\n",
    "You work as a data scientist for a cable manufacturer. Management has decided to start shipping low-resistance cables to clients around the world. To ensure that the right cables are shipped to the right countries, they would like to predict the critical temperatures of various cables based on certain observed readings.\n",
    "\n",
    "In this activity, you will train a linear regression model and compute the R2 score and the MSE. You will proceed to engineer new features using polynomial features of degree 3. You will compare the R2 score and MSE of this new model to those of the first model to determine overfitting. You will then use regularization to train a model that generalizes to previously unseen data.\n",
    "\n",
    ">Note: You will find the dataset required for the activity in the Packt GitHub repository.\n",
    "\n",
    ">The original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data).\n",
    "\n",
    ">Citation:\n",
    "\n",
    ">Hamidieh, Kam, A data-driven statistical model for predicting the critical temperature of a superconductor, Computational Materials Science, Volume 154, November 2018, pages 346-354."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n0                       4         88.944468             57.862692   \n1                       5         92.729214             58.518416   \n2                       4         88.944468             57.885242   \n3                       4         88.944468             57.873967   \n4                       4         88.944468             57.840143   \n...                   ...               ...                   ...   \n21258                   4        106.957877             53.095769   \n21259                   5         92.266740             49.021367   \n21260                   2         99.663190             95.609104   \n21261                   2         99.663190             97.095602   \n21262                   3         87.468333             86.858500   \n\n       gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n0              66.361592              36.116612             1.181795   \n1              73.132787              36.396602             1.449309   \n2              66.361592              36.122509             1.181795   \n3              66.361592              36.119560             1.181795   \n4              66.361592              36.110716             1.181795   \n...                  ...                    ...                  ...   \n21258          82.515384              43.135565             1.177145   \n21259          64.812662              32.867748             1.323287   \n21260          99.433882              95.464320             0.690847   \n21261          99.433882              96.901083             0.690847   \n21262          82.555758              80.458722             1.041270   \n\n       wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n0                     1.062396          122.90607              31.794921   \n1                     1.057755          122.90607              36.161939   \n2                     0.975980          122.90607              35.741099   \n3                     1.022291          122.90607              33.768010   \n4                     1.129224          122.90607              27.848743   \n...                        ...                ...                    ...   \n21258                 1.254119          146.88130              15.504479   \n21259                 1.571630          188.38390               7.353333   \n21260                 0.530198           13.51362              53.041104   \n21261                 0.640883           13.51362              31.115202   \n21262                 0.895229           71.75500              43.144000   \n\n       std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  \\\n0            51.968828  ...          2.257143       2.213364   \n1            47.094633  ...          2.257143       1.888175   \n2            51.968828  ...          2.271429       2.213364   \n3            51.968828  ...          2.264286       2.213364   \n4            51.968828  ...          2.242857       2.213364   \n...                ...  ...               ...            ...   \n21258        65.764081  ...          3.555556       3.223710   \n21259        69.232655  ...          2.047619       2.168944   \n21260         6.756810  ...          4.800000       4.472136   \n21261         6.756810  ...          4.690000       4.472136   \n21262        29.905282  ...          4.500000       4.762203   \n\n       wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n0               2.219783         1.368922             1.066221              1   \n1               2.210679         1.557113             1.047221              2   \n2               2.232679         1.368922             1.029175              1   \n3               2.226222         1.368922             1.048834              1   \n4               2.206963         1.368922             1.096052              1   \n...                  ...              ...                  ...            ...   \n21258           3.519911         1.377820             0.913658              1   \n21259           2.038991         1.594167             1.337246              1   \n21260           4.781762         0.686962             0.450561              1   \n21261           4.665819         0.686962             0.577601              1   \n21262           4.242641         1.054920             0.970116              3   \n\n       wtd_range_Valence  std_Valence  wtd_std_Valence  critical_temp  \n0               1.085714     0.433013         0.437059          29.00  \n1               1.128571     0.632456         0.468606          26.00  \n2               1.114286     0.433013         0.444697          19.00  \n3               1.100000     0.433013         0.440952          22.00  \n4               1.057143     0.433013         0.428809          23.00  \n...                  ...          ...              ...            ...  \n21258           2.168889     0.433013         0.496904           2.44  \n21259           0.904762     0.400000         0.212959         122.10  \n21260           3.200000     0.500000         0.400000           1.98  \n21261           2.210000     0.500000         0.462493           1.84  \n21262           1.800000     1.414214         1.500000          12.80  \n\n[21263 rows x 82 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number_of_elements</th>\n      <th>mean_atomic_mass</th>\n      <th>wtd_mean_atomic_mass</th>\n      <th>gmean_atomic_mass</th>\n      <th>wtd_gmean_atomic_mass</th>\n      <th>entropy_atomic_mass</th>\n      <th>wtd_entropy_atomic_mass</th>\n      <th>range_atomic_mass</th>\n      <th>wtd_range_atomic_mass</th>\n      <th>std_atomic_mass</th>\n      <th>...</th>\n      <th>wtd_mean_Valence</th>\n      <th>gmean_Valence</th>\n      <th>wtd_gmean_Valence</th>\n      <th>entropy_Valence</th>\n      <th>wtd_entropy_Valence</th>\n      <th>range_Valence</th>\n      <th>wtd_range_Valence</th>\n      <th>std_Valence</th>\n      <th>wtd_std_Valence</th>\n      <th>critical_temp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.862692</td>\n      <td>66.361592</td>\n      <td>36.116612</td>\n      <td>1.181795</td>\n      <td>1.062396</td>\n      <td>122.90607</td>\n      <td>31.794921</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>2.213364</td>\n      <td>2.219783</td>\n      <td>1.368922</td>\n      <td>1.066221</td>\n      <td>1</td>\n      <td>1.085714</td>\n      <td>0.433013</td>\n      <td>0.437059</td>\n      <td>29.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>92.729214</td>\n      <td>58.518416</td>\n      <td>73.132787</td>\n      <td>36.396602</td>\n      <td>1.449309</td>\n      <td>1.057755</td>\n      <td>122.90607</td>\n      <td>36.161939</td>\n      <td>47.094633</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>1.888175</td>\n      <td>2.210679</td>\n      <td>1.557113</td>\n      <td>1.047221</td>\n      <td>2</td>\n      <td>1.128571</td>\n      <td>0.632456</td>\n      <td>0.468606</td>\n      <td>26.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.885242</td>\n      <td>66.361592</td>\n      <td>36.122509</td>\n      <td>1.181795</td>\n      <td>0.975980</td>\n      <td>122.90607</td>\n      <td>35.741099</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.271429</td>\n      <td>2.213364</td>\n      <td>2.232679</td>\n      <td>1.368922</td>\n      <td>1.029175</td>\n      <td>1</td>\n      <td>1.114286</td>\n      <td>0.433013</td>\n      <td>0.444697</td>\n      <td>19.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.873967</td>\n      <td>66.361592</td>\n      <td>36.119560</td>\n      <td>1.181795</td>\n      <td>1.022291</td>\n      <td>122.90607</td>\n      <td>33.768010</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.264286</td>\n      <td>2.213364</td>\n      <td>2.226222</td>\n      <td>1.368922</td>\n      <td>1.048834</td>\n      <td>1</td>\n      <td>1.100000</td>\n      <td>0.433013</td>\n      <td>0.440952</td>\n      <td>22.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.840143</td>\n      <td>66.361592</td>\n      <td>36.110716</td>\n      <td>1.181795</td>\n      <td>1.129224</td>\n      <td>122.90607</td>\n      <td>27.848743</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.242857</td>\n      <td>2.213364</td>\n      <td>2.206963</td>\n      <td>1.368922</td>\n      <td>1.096052</td>\n      <td>1</td>\n      <td>1.057143</td>\n      <td>0.433013</td>\n      <td>0.428809</td>\n      <td>23.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21258</th>\n      <td>4</td>\n      <td>106.957877</td>\n      <td>53.095769</td>\n      <td>82.515384</td>\n      <td>43.135565</td>\n      <td>1.177145</td>\n      <td>1.254119</td>\n      <td>146.88130</td>\n      <td>15.504479</td>\n      <td>65.764081</td>\n      <td>...</td>\n      <td>3.555556</td>\n      <td>3.223710</td>\n      <td>3.519911</td>\n      <td>1.377820</td>\n      <td>0.913658</td>\n      <td>1</td>\n      <td>2.168889</td>\n      <td>0.433013</td>\n      <td>0.496904</td>\n      <td>2.44</td>\n    </tr>\n    <tr>\n      <th>21259</th>\n      <td>5</td>\n      <td>92.266740</td>\n      <td>49.021367</td>\n      <td>64.812662</td>\n      <td>32.867748</td>\n      <td>1.323287</td>\n      <td>1.571630</td>\n      <td>188.38390</td>\n      <td>7.353333</td>\n      <td>69.232655</td>\n      <td>...</td>\n      <td>2.047619</td>\n      <td>2.168944</td>\n      <td>2.038991</td>\n      <td>1.594167</td>\n      <td>1.337246</td>\n      <td>1</td>\n      <td>0.904762</td>\n      <td>0.400000</td>\n      <td>0.212959</td>\n      <td>122.10</td>\n    </tr>\n    <tr>\n      <th>21260</th>\n      <td>2</td>\n      <td>99.663190</td>\n      <td>95.609104</td>\n      <td>99.433882</td>\n      <td>95.464320</td>\n      <td>0.690847</td>\n      <td>0.530198</td>\n      <td>13.51362</td>\n      <td>53.041104</td>\n      <td>6.756810</td>\n      <td>...</td>\n      <td>4.800000</td>\n      <td>4.472136</td>\n      <td>4.781762</td>\n      <td>0.686962</td>\n      <td>0.450561</td>\n      <td>1</td>\n      <td>3.200000</td>\n      <td>0.500000</td>\n      <td>0.400000</td>\n      <td>1.98</td>\n    </tr>\n    <tr>\n      <th>21261</th>\n      <td>2</td>\n      <td>99.663190</td>\n      <td>97.095602</td>\n      <td>99.433882</td>\n      <td>96.901083</td>\n      <td>0.690847</td>\n      <td>0.640883</td>\n      <td>13.51362</td>\n      <td>31.115202</td>\n      <td>6.756810</td>\n      <td>...</td>\n      <td>4.690000</td>\n      <td>4.472136</td>\n      <td>4.665819</td>\n      <td>0.686962</td>\n      <td>0.577601</td>\n      <td>1</td>\n      <td>2.210000</td>\n      <td>0.500000</td>\n      <td>0.462493</td>\n      <td>1.84</td>\n    </tr>\n    <tr>\n      <th>21262</th>\n      <td>3</td>\n      <td>87.468333</td>\n      <td>86.858500</td>\n      <td>82.555758</td>\n      <td>80.458722</td>\n      <td>1.041270</td>\n      <td>0.895229</td>\n      <td>71.75500</td>\n      <td>43.144000</td>\n      <td>29.905282</td>\n      <td>...</td>\n      <td>4.500000</td>\n      <td>4.762203</td>\n      <td>4.242641</td>\n      <td>1.054920</td>\n      <td>0.970116</td>\n      <td>3</td>\n      <td>1.800000</td>\n      <td>1.414214</td>\n      <td>1.500000</td>\n      <td>12.80</td>\n    </tr>\n  </tbody>\n</table>\n<p>21263 rows × 82 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Dataset/superconduct/train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 21263 entries, 0 to 21262\nData columns (total 82 columns):\n #   Column                           Non-Null Count  Dtype  \n---  ------                           --------------  -----  \n 0   number_of_elements               21263 non-null  int64  \n 1   mean_atomic_mass                 21263 non-null  float64\n 2   wtd_mean_atomic_mass             21263 non-null  float64\n 3   gmean_atomic_mass                21263 non-null  float64\n 4   wtd_gmean_atomic_mass            21263 non-null  float64\n 5   entropy_atomic_mass              21263 non-null  float64\n 6   wtd_entropy_atomic_mass          21263 non-null  float64\n 7   range_atomic_mass                21263 non-null  float64\n 8   wtd_range_atomic_mass            21263 non-null  float64\n 9   std_atomic_mass                  21263 non-null  float64\n 10  wtd_std_atomic_mass              21263 non-null  float64\n 11  mean_fie                         21263 non-null  float64\n 12  wtd_mean_fie                     21263 non-null  float64\n 13  gmean_fie                        21263 non-null  float64\n 14  wtd_gmean_fie                    21263 non-null  float64\n 15  entropy_fie                      21263 non-null  float64\n 16  wtd_entropy_fie                  21263 non-null  float64\n 17  range_fie                        21263 non-null  float64\n 18  wtd_range_fie                    21263 non-null  float64\n 19  std_fie                          21263 non-null  float64\n 20  wtd_std_fie                      21263 non-null  float64\n 21  mean_atomic_radius               21263 non-null  float64\n 22  wtd_mean_atomic_radius           21263 non-null  float64\n 23  gmean_atomic_radius              21263 non-null  float64\n 24  wtd_gmean_atomic_radius          21263 non-null  float64\n 25  entropy_atomic_radius            21263 non-null  float64\n 26  wtd_entropy_atomic_radius        21263 non-null  float64\n 27  range_atomic_radius              21263 non-null  int64  \n 28  wtd_range_atomic_radius          21263 non-null  float64\n 29  std_atomic_radius                21263 non-null  float64\n 30  wtd_std_atomic_radius            21263 non-null  float64\n 31  mean_Density                     21263 non-null  float64\n 32  wtd_mean_Density                 21263 non-null  float64\n 33  gmean_Density                    21263 non-null  float64\n 34  wtd_gmean_Density                21263 non-null  float64\n 35  entropy_Density                  21263 non-null  float64\n 36  wtd_entropy_Density              21263 non-null  float64\n 37  range_Density                    21263 non-null  float64\n 38  wtd_range_Density                21263 non-null  float64\n 39  std_Density                      21263 non-null  float64\n 40  wtd_std_Density                  21263 non-null  float64\n 41  mean_ElectronAffinity            21263 non-null  float64\n 42  wtd_mean_ElectronAffinity        21263 non-null  float64\n 43  gmean_ElectronAffinity           21263 non-null  float64\n 44  wtd_gmean_ElectronAffinity       21263 non-null  float64\n 45  entropy_ElectronAffinity         21263 non-null  float64\n 46  wtd_entropy_ElectronAffinity     21263 non-null  float64\n 47  range_ElectronAffinity           21263 non-null  float64\n 48  wtd_range_ElectronAffinity       21263 non-null  float64\n 49  std_ElectronAffinity             21263 non-null  float64\n 50  wtd_std_ElectronAffinity         21263 non-null  float64\n 51  mean_FusionHeat                  21263 non-null  float64\n 52  wtd_mean_FusionHeat              21263 non-null  float64\n 53  gmean_FusionHeat                 21263 non-null  float64\n 54  wtd_gmean_FusionHeat             21263 non-null  float64\n 55  entropy_FusionHeat               21263 non-null  float64\n 56  wtd_entropy_FusionHeat           21263 non-null  float64\n 57  range_FusionHeat                 21263 non-null  float64\n 58  wtd_range_FusionHeat             21263 non-null  float64\n 59  std_FusionHeat                   21263 non-null  float64\n 60  wtd_std_FusionHeat               21263 non-null  float64\n 61  mean_ThermalConductivity         21263 non-null  float64\n 62  wtd_mean_ThermalConductivity     21263 non-null  float64\n 63  gmean_ThermalConductivity        21263 non-null  float64\n 64  wtd_gmean_ThermalConductivity    21263 non-null  float64\n 65  entropy_ThermalConductivity      21263 non-null  float64\n 66  wtd_entropy_ThermalConductivity  21263 non-null  float64\n 67  range_ThermalConductivity        21263 non-null  float64\n 68  wtd_range_ThermalConductivity    21263 non-null  float64\n 69  std_ThermalConductivity          21263 non-null  float64\n 70  wtd_std_ThermalConductivity      21263 non-null  float64\n 71  mean_Valence                     21263 non-null  float64\n 72  wtd_mean_Valence                 21263 non-null  float64\n 73  gmean_Valence                    21263 non-null  float64\n 74  wtd_gmean_Valence                21263 non-null  float64\n 75  entropy_Valence                  21263 non-null  float64\n 76  wtd_entropy_Valence              21263 non-null  float64\n 77  range_Valence                    21263 non-null  int64  \n 78  wtd_range_Valence                21263 non-null  float64\n 79  std_Valence                      21263 non-null  float64\n 80  wtd_std_Valence                  21263 non-null  float64\n 81  critical_temp                    21263 non-null  float64\ndtypes: float64(79), int64(3)\nmemory usage: 13.3 MB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(21263, 81) (21263, 1)\n"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df[['critical_temp']]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training - test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baseline linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "scoring = ['r2', 'neg_mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.07395506, 0.05996394, 0.09194851]),\n 'score_time': array([0.02198792, 0.0159905 , 0.02798486]),\n 'test_r2': array([0.72165815, 0.71293602, 0.72115141]),\n 'test_neg_mean_squared_error': array([-329.00478125, -328.9835369 , -322.66484685])}"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "steps1 = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('lr', LinearRegression())\n",
    "]\n",
    "pipeline1 = Pipeline(steps1)\n",
    "\n",
    "result1 = cross_validate(pipeline1, X_test, y_test, cv=kf, scoring=scoring)\n",
    "\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "R2: 0.7185818632099563, MSE: 326.88438833334527\n"
    }
   ],
   "source": [
    "print(f\"R2: {result1['test_r2'].mean()}, MSE: {-result1['test_neg_mean_squared_error'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline to engineer polynomial features and train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([32.79696918, 32.63448501, 35.21093869]),\n 'score_time': array([0.13292384, 0.09594512, 0.14291692]),\n 'test_r2': array([-1.45682028e+19, -6.27565877e+18, -9.91687265e+18]),\n 'test_neg_mean_squared_error': array([-1.72198625e+22, -7.19208468e+21, -1.14751387e+22])}"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "steps2 = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),    \n",
    "    ('lr', LinearRegression())\n",
    "]\n",
    "pipeline2 = Pipeline(steps2)\n",
    "\n",
    "result2 = cross_validate(pipeline2, X_test, y_test, cv=kf, scoring=scoring)\n",
    "\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "R2: -1.0253578068999096e+19, MSE: 1.1962361954314085e+22\n"
    }
   ],
   "source": [
    "print(f\"R2: {result2['test_r2'].mean()}, MSE: {-result2['test_neg_mean_squared_error'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a ridge or lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([14.45138836, 14.75088143, 12.95537543]),\n 'score_time': array([0.15790987, 0.12392926, 0.14891315]),\n 'test_r2': array([0.78551718, 0.77490672, 0.78465957]),\n 'test_neg_mean_squared_error': array([-253.52232644, -257.96334529, -249.17747729])}"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "steps3 = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),    \n",
    "    ('lr', Lasso(alpha=0.01))\n",
    "]\n",
    "pipeline3 = Pipeline(steps3)\n",
    "\n",
    "result3 = cross_validate(pipeline3, X_test, y_test, cv=kf, scoring=scoring)\n",
    "\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "R2: 0.7816944898129496, MSE: 253.55438300603456\n"
    }
   ],
   "source": [
    "print(f\"R2: {result3['test_r2'].mean()}, MSE: {-result3['test_neg_mean_squared_error'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([2.68798995, 2.15975904, 1.92189693]),\n 'score_time': array([0.17989564, 0.11593509, 0.1059413 ]),\n 'test_r2': array([0.81006689, 0.79403683, 0.8012763 ]),\n 'test_neg_mean_squared_error': array([-224.50415722, -236.03968931, -229.94971716])}"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "steps4 = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),    \n",
    "    ('lr', Ridge(alpha=0.9))\n",
    "]\n",
    "pipeline4 = Pipeline(steps4)\n",
    "\n",
    "result4 = cross_validate(pipeline4, X_test, y_test, cv=kf, scoring=scoring)\n",
    "\n",
    "result4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "R2: 0.8017933383599504, MSE: 230.1645212291246\n"
    }
   ],
   "source": [
    "print(f\"R2: {result4['test_r2'].mean()}, MSE: {-result4['test_neg_mean_squared_error'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8223554009470894"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "pipeline = Pipeline(steps4)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "206.4210776548909"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "mean_squared_error(y_test, model.predict(X_test))"
   ]
  }
 ]
}