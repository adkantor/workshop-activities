{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600874178135",
   "display_name": "Python 3.7.2 64-bit ('dsw': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Comparison of Advanced Ensemble Techniques\n",
    "\n",
    "Scenario: You have tried the benchmark model on the credit card dataset and have got some benchmark metrics. Having learned some advanced ensemble techniques, you want to determine which technique to use for the credit card approval dataset.\n",
    "\n",
    "In this activity, you will use all three advanced techniques and compare the results before selecting your final technique."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from mlxtend.classifier import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data using pandas\n",
    "credData = pd.read_csv('../Dataset/crx.data', sep=\",\", header = None, na_values = \"?\")\n",
    "\n",
    "# Changing the Classes to 1 & 0\n",
    "credData.loc[credData[15] == '+' , 15] = 1\n",
    "credData.loc[credData[15] == '-' , 15] = 0\n",
    "# Dropping all the rows with na values\n",
    "newcred = credData.dropna(axis = 0)\n",
    "# Seperating the categorical variables to make dummy variables\n",
    "credCat = pd.get_dummies(newcred[[0,3,4,5,6,8,9,11,12]])\n",
    "# Seperating the numerical variables\n",
    "credNum = newcred[[1,2,7,10,13,14]]\n",
    "# Making the X variable which is a concatenation of categorical and numerical data\n",
    "X = pd.concat([credCat,credNum],axis = 1)\n",
    "# Seperating the label as y variable\n",
    "y = newcred[15].astype('int')\n",
    "\n",
    "# Normalising the data sets\n",
    "minmaxScaler = preprocessing.MinMaxScaler()\n",
    "X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))\n",
    "\n",
    "# Splitting the data set to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tran, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "source": [
    "## Implement the bagging technique with the base learner as the logistic regression model. \n",
    "\n",
    "In the bagging classifier, define n_estimators = 15, max_samples = 0.7, and max_features = 0.8. \n",
    "\n",
    "Fit the model on the training set, generate the predictions, and print the confusion matrix and the classification report."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = LogisticRegression()\n",
    "bagging_ensemble_model = BaggingClassifier(\n",
    "    base_estimator=base_estimator, \n",
    "    n_estimators=15, \n",
    "    max_samples=0.7,\n",
    "    max_features=0.8,\n",
    "    random_state=123,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\nBuilding estimator 1 of 15 for this parallel run (total 15)...\nBuilding estimator 2 of 15 for this parallel run (total 15)...\nBuilding estimator 3 of 15 for this parallel run (total 15)...\nBuilding estimator 4 of 15 for this parallel run (total 15)...\nBuilding estimator 5 of 15 for this parallel run (total 15)...\nBuilding estimator 6 of 15 for this parallel run (total 15)...\nBuilding estimator 7 of 15 for this parallel run (total 15)...\nBuilding estimator 8 of 15 for this parallel run (total 15)...\nBuilding estimator 9 of 15 for this parallel run (total 15)...\nBuilding estimator 10 of 15 for this parallel run (total 15)...\nBuilding estimator 11 of 15 for this parallel run (total 15)...\nBuilding estimator 12 of 15 for this parallel run (total 15)...\nBuilding estimator 13 of 15 for this parallel run (total 15)...\nBuilding estimator 14 of 15 for this parallel run (total 15)...\nBuilding estimator 15 of 15 for this parallel run (total 15)...\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BaggingClassifier(base_estimator=LogisticRegression(), max_features=0.8,\n                  max_samples=0.7, n_estimators=15, random_state=123,\n                  verbose=2)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "bagging_ensemble_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
    }
   ],
   "source": [
    "y_pred = bagging_ensemble_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.898\n\n[[94 13]\n [ 7 82]]\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.88      0.90       107\n           1       0.86      0.92      0.89        89\n\n    accuracy                           0.90       196\n   macro avg       0.90      0.90      0.90       196\nweighted avg       0.90      0.90      0.90       196\n\n"
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "## Implement boosting with random forest as the base learner. \n",
    "\n",
    "In the AdaBoostClassifier, define n_estimators = 300. \n",
    "\n",
    "Fit the model on the training set, generate the predictions, and print the confusion matrix and classification report."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = RandomForestClassifier(random_state=123)\n",
    "boosting_ensemble_model = AdaBoostClassifier(\n",
    "    base_estimator=base_estimator, \n",
    "    n_estimators=300,\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "AdaBoostClassifier(base_estimator=RandomForestClassifier(random_state=123),\n                   n_estimators=300, random_state=123)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "boosting_ensemble_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = boosting_ensemble_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.898\n\n[[95 12]\n [ 8 81]]\n\n\n              precision    recall  f1-score   support\n\n           0       0.92      0.89      0.90       107\n           1       0.87      0.91      0.89        89\n\n    accuracy                           0.90       196\n   macro avg       0.90      0.90      0.90       196\nweighted avg       0.90      0.90      0.90       196\n\n"
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "## Implement the stacking technique. \n",
    "\n",
    "Make the KNN and logistic regression models base learners and random forest a meta learner. \n",
    "\n",
    "Fit the model on the training set, generate the predictions, and print the confusion matrix and classification report."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "lr = LogisticRegression(random_state=123)\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "stacking_ensemble_model = StackingClassifier(\n",
    "    classifiers=[knn, lr],\n",
    "    meta_classifier=rf,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 2 classifiers...\nFitting classifier1: kneighborsclassifier (1/2)\nKNeighborsClassifier()\nFitting classifier2: logisticregression (2/2)\nLogisticRegression(random_state=123)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "StackingClassifier(classifiers=[KNeighborsClassifier(),\n                                LogisticRegression(random_state=123)],\n                   meta_classifier=RandomForestClassifier(random_state=123),\n                   verbose=2)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "stacking_ensemble_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacking_ensemble_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.867\n\n[[99  8]\n [18 71]]\n\n\n              precision    recall  f1-score   support\n\n           0       0.85      0.93      0.88       107\n           1       0.90      0.80      0.85        89\n\n    accuracy                           0.87       196\n   macro avg       0.87      0.86      0.86       196\nweighted avg       0.87      0.87      0.87       196\n\n"
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}